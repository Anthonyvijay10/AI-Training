{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3947b02b-9647-4de7-a942-2301945b8f9f",
   "metadata": {},
   "source": [
    "## Explanation of the MLP Forward and Backward Pass\r\n",
    "\r\n",
    "### 1. Define Model Architecture and Initialize Parameters\r\n",
    "\r\n",
    "- We define the dimensions of the input, hidden, and output layers:\r\n",
    "  - `input_size = 10`\r\n",
    "  - `hidden_size = 20`\r\n",
    "  - `output_size = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777a2e32-f332-4f11-9140-7ab35f819f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the dimensions of the input, hidden, and output layers\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b11d4-56db-4e13-a1df-854544c8b0e0",
   "metadata": {},
   "source": [
    "- We initialize the weights and biases for the MLP using PyTorch tensors:\n",
    "  - `w1`: Weight matrix for the first layer (size: `input_size x hidden_size`)\n",
    "  - `b1`: Bias vector for the first layer (size: `1 x hidden_size`)\n",
    "  - `w2`: Weight matrix for the second layer (size: `hidden_size x output_size`)\n",
    "  - `b2`: Bias vector for the second layer (size: `1 x output_size`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb06d479-cd6a-4f98-9ad9-2c596dad7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases for the MLP\n",
    "w1 = torch.randn(input_size, hidden_size, requires_grad=True)  # Weight matrix for first layer\n",
    "b1 = torch.randn(1, hidden_size, requires_grad=True)  # Bias vector for first layer\n",
    "w2 = torch.randn(hidden_size, output_size, requires_grad=True)  # Weight matrix for second layer\n",
    "b2 = torch.randn(1, output_size, requires_grad=True)  # Bias vector for second layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3104d-f14f-42d1-9b29-bb610155d458",
   "metadata": {},
   "source": [
    "### 2. Forward Pass\n",
    "\n",
    "- We define the input tensor (`input_tensor`) and target output tensor (`target_output`).\n",
    "- We perform the forward pass to compute the predicted output tensor (`output_tensor`):\n",
    "  - Linear transformation and ReLU activation for the first hidden layer\n",
    "  - Linear transformation for the output layer\n",
    "- We compute the mean squared error loss (`loss`) between the predicted output and the target output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c30492-8802-4e91-a068-711d5d42959e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8218, -0.5395, -0.3298,  0.6342,  0.2875,  1.3541, -1.7218, -0.9154,\n",
       "          0.4241,  0.3175]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab98437-9334-4a6e-b2e1-622df5f8e2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.7385, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "hidden_output = torch.matmul(input_tensor, w1) + b1\n",
    "hidden_output = F.relu(hidden_output)\n",
    "output_tensor = torch.matmul(hidden_output, w2) + b2\n",
    "loss = F.mse_loss(output_tensor, target_output)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aa69cc-1b89-4005-b502-c690028067d7",
   "metadata": {},
   "source": [
    "### 3. Backward Pass\n",
    "\n",
    "- We compute the gradients of the loss with respect to each parameter using the chain rule and manual differentiation:\n",
    "  - Gradient of the loss with respect to the output tensor (`output_grad`)\n",
    "  - Gradients of the loss with respect to `w2` and `b2`\n",
    "  - Gradient of the loss with respect to the hidden layer output\n",
    "  - Gradient of the loss with respect to the hidden layer activation (ReLU derivative)\n",
    "  - Gradients of the loss with respect to `w1` and `b1`\n",
    "- We print the computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4391901b-e06d-4675-8323-2c0c6d8bbd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of w1:\n",
      "tensor([[  4.1775,   1.5801,   9.7490,  -0.4912,  -4.4792,  -4.2421,  -1.9519,\n",
      "          -4.9478,  -0.9522,  -4.6351,   3.7296,   3.1289,   0.6668,  -2.6608,\n",
      "          -1.2238,   3.9295,   2.8678,   7.4332,  -3.9013,   0.9898],\n",
      "        [  2.7425,   1.0373,   6.4000,  -0.3225,  -2.9405,  -2.7849,  -1.2814,\n",
      "          -3.2481,  -0.6251,  -3.0429,   2.4484,   2.0541,   0.4377,  -1.7467,\n",
      "          -0.8034,   2.5796,   1.8827,   4.8797,  -2.5611,   0.6498],\n",
      "        [  1.6765,   0.6341,   3.9124,  -0.1971,  -1.7976,  -1.7024,  -0.7833,\n",
      "          -1.9856,  -0.3821,  -1.8601,   1.4968,   1.2557,   0.2676,  -1.0678,\n",
      "          -0.4911,   1.5770,   1.1509,   2.9830,  -1.5657,   0.3972],\n",
      "        [ -3.2241,  -1.2195,  -7.5240,   0.3791,   3.4569,   3.2739,   1.5064,\n",
      "           3.8185,   0.7349,   3.5772,  -2.8784,  -2.4148,  -0.5146,   2.0535,\n",
      "           0.9445,  -3.0327,  -2.2133,  -5.7367,   3.0109,  -0.7639],\n",
      "        [ -1.4613,  -0.5527,  -3.4103,   0.1718,   1.5669,   1.4839,   0.6828,\n",
      "           1.7308,   0.3331,   1.6214,  -1.3046,  -1.0945,  -0.2332,   0.9308,\n",
      "           0.4281,  -1.3746,  -1.0032,  -2.6002,   1.3647,  -0.3462],\n",
      "        [ -6.8833,  -2.6035, -16.0635,   0.8093,   7.3804,   6.9897,   3.2161,\n",
      "           8.1525,   1.5689,   7.6373,  -6.1453,  -5.1556,  -1.0986,   4.3842,\n",
      "           2.0165,  -6.4746,  -4.7253, -12.2476,   6.4282,  -1.6309],\n",
      "        [  8.7525,   3.3105,  20.4256,  -1.0291,  -9.3846,  -8.8878,  -4.0895,\n",
      "         -10.3663,  -1.9950,  -9.7112,   7.8141,   6.5556,   1.3969,  -5.5747,\n",
      "          -2.5641,   8.2328,   6.0085,  15.5736,  -8.1738,   2.0738],\n",
      "        [  4.6535,   1.7601,  10.8598,  -0.5472,  -4.9896,  -4.7255,  -2.1743,\n",
      "          -5.5115,  -1.0607,  -5.1632,   4.1546,   3.4855,   0.7427,  -2.9639,\n",
      "          -1.3633,   4.3772,   3.1946,   8.2801,  -4.3458,   1.1026],\n",
      "        [ -2.1558,  -0.8154,  -5.0311,   0.2535,   2.3115,   2.1892,   1.0073,\n",
      "           2.5533,   0.4914,   2.3920,  -1.9247,  -1.6147,  -0.3441,   1.3731,\n",
      "           0.6316,  -2.0278,  -1.4800,  -3.8359,   2.0133,  -0.5108],\n",
      "        [ -1.6139,  -0.6104,  -3.7664,   0.1898,   1.7305,   1.6389,   0.7541,\n",
      "           1.9115,   0.3679,   1.7907,  -1.4409,  -1.2088,  -0.2576,   1.0280,\n",
      "           0.4728,  -1.5181,  -1.1079,  -2.8717,   1.5072,  -0.3824]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "Gradients of b1:\n",
      "tensor([[ -5.0834,  -1.9227, -11.8631,   0.5977,   5.4505,   5.1620,   2.3752,\n",
      "           6.0207,   1.1587,   5.6403,  -4.5384,  -3.8075,  -0.8113,   3.2378,\n",
      "           1.4892,  -4.7816,  -3.4897,  -9.0451,   4.7473,  -1.2045]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Gradients of w2:\n",
      "tensor([[-22.5609],\n",
      "        [ -5.4731],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [ -9.7010],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [ -0.6847],\n",
      "        [-13.1944],\n",
      "        [  0.0000],\n",
      "        [ -1.8313],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [-18.3225],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000]], grad_fn=<MmBackward0>)\n",
      "Gradients of b2:\n",
      "tensor([[-4.7910]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "loss_grad = torch.tensor(1.0)  # Gradient of the loss with respect to itself\n",
    "\n",
    "# Gradient of loss with respect to output tensor\n",
    "output_grad = 2 * (output_tensor - target_output) / output_tensor.size(0)\n",
    "\n",
    "# Gradient of loss with respect to w2 and b2\n",
    "w2_grad = torch.matmul(hidden_output.t(), output_grad)\n",
    "b2_grad = output_grad.sum(0, keepdim=True)\n",
    "\n",
    "# Gradient of loss with respect to hidden layer output\n",
    "hidden_output_grad = torch.matmul(output_grad, w2.t())\n",
    "\n",
    "# Gradient of loss with respect to hidden layer activation\n",
    "hidden_output_grad[hidden_output < 0] = 0  # ReLU derivative (ReLU'(x) = 0 if x < 0)\n",
    "\n",
    "# Gradient of loss with respect to w1 and b1\n",
    "w1_grad = torch.matmul(input_tensor.t(), hidden_output_grad)\n",
    "b1_grad = hidden_output_grad.sum(0, keepdim=True)\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients of w1:\")\n",
    "print(w1_grad)\n",
    "print(\"Gradients of b1:\")\n",
    "print(b1_grad)\n",
    "print(\"Gradients of w2:\")\n",
    "print(w2_grad)\n",
    "print(\"Gradients of b2:\")\n",
    "print(b2_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6c363-0125-4f28-8fc5-103dfe11db98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JKR",
   "language": "python",
   "name": "jkr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
