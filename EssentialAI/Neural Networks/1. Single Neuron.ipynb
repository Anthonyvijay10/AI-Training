{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e4c11b-8f3c-41cd-a3d5-a5cc8a09d92d",
   "metadata": {},
   "source": [
    "## Single Neuron with ReLU Activation ##\n",
    "\n",
    "In neural networks, a single neuron is a fundamental building block that performs a linear transformation followed by an activation function. The linear transformation involves multiplying the input features by corresponding weights and adding a bias term. The activation function introduces non-linearity to the neuron's output, enabling it to learn complex patterns in the data.\n",
    "1. **Input Layer**:\n",
    "   - The input layer represents the input features to the neuron. It's typically represented as a row vector where each element corresponds to an input feature:\n",
    "     $$\n",
    "     \\text{Input Layer} = \\begin{bmatrix}\n",
    "     x_1 & x_2 & \\cdots & x_n\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "   - Here, $$ x_1, x_2, \\ldots, x_n $$ represent the individual input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650b5a41-cd21-4d85-a02f-4c5c02b6ec65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define input tensor (1 sample, 4 features)\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fed91-c912-4685-8bc2-62eff0b0d034",
   "metadata": {},
   "source": [
    "2. **Weights**:\n",
    "   - Weights are the parameters of the neuron that are learned during the training process. Each weight corresponds to a connection between an input feature and the neuron.\n",
    "   - Weights are typically represented as a column vector:\n",
    "     $$\n",
    "     \\text{Weights} = \\begin{bmatrix}\n",
    "     w_1 \\\\\n",
    "     w_2 \\\\\n",
    "     \\vdots \\\\\n",
    "     w_n\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "   - Here, $$ w_1, w_2, \\ldots, w_n $$ represent the individual weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf79cee-44da-409b-8a20-dd35be06922c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.2000],\n",
       "        [0.3000],\n",
       "        [0.4000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights \n",
    "weights = torch.tensor([[0.1], [0.2], [0.3], [0.4]])\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa049e6-4efe-4720-8118-e276c6244a63",
   "metadata": {},
   "source": [
    "3. **Bias**:\n",
    "   - The bias term is another parameter of the neuron. It's added to the weighted sum of inputs to shift the activation function.\n",
    "   - The bias is represented as a scalar value:\n",
    "     $$\n",
    "     \\text{Bias} = b\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121372cf-282c-47e1-87ad-25ccca08e1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias\n",
    "bias = torch.tensor([0.5])\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb321b-671b-4637-b192-a52c0483b803",
   "metadata": {},
   "source": [
    "4. **Linear Transformation**:\n",
    "   - This represents the result of the linear transformation of the input by the neuron, including the weighted sum of inputs and the bias term.\n",
    "   - It's computed by multiplying the input layer by the weights and then adding the bias term:\n",
    "     $$\n",
    "     \\text{Linear Output} = \\begin{bmatrix}\n",
    "     x_1w_1 + x_2w_2 + \\cdots + x_nw_n + b\n",
    "     \\end{bmatrix}\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bca376-3efa-49b3-8ebf-9f933d4512d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear transformation (input layer -> neuron)\n",
    "linear_output = torch.matmul(x, weights) + bias\n",
    "linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8bcfbe-9532-445b-b573-95881f839862",
   "metadata": {},
   "source": [
    "5. **ReLU Activation**:\n",
    "   - ReLU (Rectified Linear Unit) is an activation function commonly used in neural networks to introduce non-linearity.\n",
    "   - It applies an element-wise operation to the output of Step 1, replacing any negative values with zero:\n",
    "     $$\n",
    "     \\text{ReLU}(\\text{Linear Output}) = \\begin{bmatrix}\n",
    "     \\max(0, x_1w_1 + x_2w_2 + \\cdots + x_nw_n + b)\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "   - This ensures that the output of the neuron is always non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6e8cfa-b214-45df-8282-041aa15ec5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU output: tensor([[3.5000]])\n"
     ]
    }
   ],
   "source": [
    "# ReLU activation\n",
    "relu_output = nn.functional.relu(linear_output)\n",
    "\n",
    "# Output\n",
    "print(\"ReLU output:\", relu_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ecbfd4-eeec-4c72-bbfe-c68146e273c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JKR",
   "language": "python",
   "name": "jkr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
